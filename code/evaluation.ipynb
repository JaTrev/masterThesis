{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"evaluation.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMfrZFM54nPl0fEyo66jpsN"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"jQXflsqcP0Hc","colab_type":"code","colab":{}},"source":["from itertools import combinations \n","from collections import Counter\n","from sklearn import metrics\n","import numpy as np\n","import gensim"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hhPbHwv1P8yV","colab_type":"code","colab":{}},"source":["def getWordCount(docs:list) -> dict:\n","  \"\"\"\n","  Return a count of all words in the documents.\n","\n","  Parameters:\n","    docs (list): list of documents\n","\n","  Returns: \n","    dict: absolute frequency of each word\n","\n","  \"\"\"\n","\n","  temp = []\n","  for doc in docs:\n","    temp.extend(doc)\n","\n","  return Counter(temp)\n","\n","\n","\n","def getClusterWords(docs_cleaned:list, words:list, n_concept:int, \n","                    vocab_dict:dict, word_labels=None, \n","                    doc_labels=None)-> list:\n","  \"\"\"\n","  Function returns a list of words for each cluster.\n","\n","  Parameters:\n","    docs_cleaned(list): list of documents\n","    words(list): list of words and their associated ids\n","    n_concept(int): number of clusters to create\n","    vocab_dict(dict): dictionary of the relevant words\n","    word_labels(list): (optional) indicating the cluster of each word\n","    doc_labels(list): (optional) indicating the cluster of each document\n","\n","  Returns:\n","    A list of words for each cluster.\n","  \"\"\"\n","  \n","  labels = None\n","  if isinstance(word_labels, list):\n","    labels = word_labels\n","  elif isinstance(doc_labels, list):\n","    labels = doc_labels\n","  else:\n","    print(\"Need to add word_labels or doc_labels\")\n","    return\n","\n","  \n","  word_counts = getWordCount(docs_cleaned)\n","  vocab =  list(vocab_dict.values())\n","  corpus = [vocab_dict.doc2bow(text) for text in docs_cleaned]\n","  tfidf = gensim.models.TfidfModel(corpus, id2word=vocab_dict.id2token)\n","  corpus_tfidf = tfidf[corpus]\n","\n","\n","  # group all words in the vocab by clusters\n","  cluster_words = [[] for i in range(n_concept)]\n","\n","  if isinstance(doc_labels, list) :\n","    # working on document level\n","    for doc_id, doc in enumerate(docs_cleaned):\n","      cluster_words[doc_labels[doc_id]].extend([w for w in doc \n","                                                if w in vocab])\n","  else:\n","    # working on word level\n","    for word_id, word in enumerate(words):\n","      if word in vocab:\n","        # make sure the word is in the vocabulary\n","        cluster_words[word_labels[word_id]].append(word)\n","\n","\n","  # calculate absolute frequency * idf for cluster_words:\n","  cluster_words_freq = [[] for i in range(n_concept)]\n","  for cluster_id, cluster_list in enumerate(cluster_words):\n","\n","    # remove any duplicates\n","    cluster_list = list(set(cluster_list))\n","\n","    for w in cluster_list:\n","\n","      # weight words by their absolute frequency and their idf\n","      w_absolute_count = word_counts.get(w)\n","      w_idf = corpus_tfidf.obj.idfs[dictionary.token2id[w]]\n","      cluster_words_freq[cluster_id].append([w, w_absolute_count * w_idf])\n","\n","\n","  \"\"\"\n","  # assign absolute frequence to each word of each cluster\n","  cluster_words_freq = [[] for i in range(n_concept)]\n","  for cluster_id, cluster_list in enumerate(cluster_words):\n","\n","    for w in cluster_list:\n","\n","      w_count = word_counts.get(w)\n","      cluster_words_freq[cluster_id].append([w, w_count])\n","  \"\"\"\n","\n","\n","  # sort each word by their absolute frequency\n","  cluster_words_sorted = [[] for i in range(n_concept)]\n","  for cluster_id, cluster_list in enumerate(cluster_words_freq):\n","    \n","\n","    sorted_list = sorted(cluster_list, key= lambda x: x[1], reverse=True)\n","    cluster_words_sorted[cluster_id].extend(list(set(map(\n","        itemgetter(0), sorted_list ))))\n","  \n","  return cluster_words_sorted\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FGYLnxFVALHk","colab_type":"code","colab":{}},"source":["from sklearn.metrics.pairwise import cosine_similarity\n","from math import log2, log10, log1p\n","import numpy as np\n","\n","def coherenceScore(cluster_words_sorted:list, \n","                   w2v_model=None, top_n_topic_words=10,\n","                   n_clusters=20) -> float:\n","  \"\"\"\n","  Calculates the coherence score, using embeddings and cos similarity.\n","\n","  Parameters:\n","    cluster_words_sorted(list): list of sorted words for each cluster\n","    w2v_model: word2vec model used for vectorization of the model\n","    top_n_topic_words(int): number of words used to describe each cluster\n","    n_clusters(int): number of cluters\n","\n","  Returns:\n","    Avergage coherence score across all topics.\n","\n","  \"\"\"\n","  assert n_clusters == len(cluster_words_sorted), (\"len(cluster_words_sorted) \"\n","                                                    \"!= n_clusters\")\n","  per_topic_cs = [0 for _ in range(n_clusters)]\n","  for topic_id, topic_word_list in enumerate(cluster_words_sorted):\n","\n","    top_words = topic_word_list[:top_n_topic_words]\n","\n","    # calculate similarity for each pair of terms\n","    pair_scores = 0\n","    for pair in combinations(top_words, 2):\n","\n","      pair_scores += log1p(cosine_similarity(\n","          w2v_model.wv.get_vector(pair[0]).reshape(1, -1), \n","          w2v_model.wv.get_vector(pair[1]).reshape(1, -1)))\n","    \n","\n","    # devide the score by the number of pairs\n","    per_topic_cs[topic_id] =  pair_scores / sum(range(top_n_topic_words)) \n","\n","\n","  # return the mean score across all topics \n","  return np.average(per_topic_cs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MoJIbAC6jXie","colab_type":"code","colab":{}},"source":["from s_dbw import S_Dbw\n","def s_Dbw_validity_index(data:list, labels_pred:list, \n","                         centers_id:list)-> float:\n","  \"\"\"\n","  Function return the s_Dbw validity index.\n","\n","  Parameters:\n","    data(list): list of vectors\n","    labels_pred: the predicted labels of each doc (document-level)\n","    centers_id: ids of the vectors of the cluster centers\n","\n","  Returns:\n","    s Dbw value\n","  \"\"\"\n","  \n","  return S_Dbw(data, labels_pred, centers_id, method='Halkidi', \n","               alg_noise='bind', centr='mean',nearest_centr=True, \n","               metric='euclidean')\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3K4Y4rP6jXmG","colab_type":"code","colab":{}},"source":["def silhoutteCoefficient(data:list, labels_pred:list) -> float: \n","  \"\"\"\n","  Function calculates the silhouette coefficient.\n","\n","  Parameters:\n","    data(list): list of documents\n","    lavels_pred: the predicted labels of each doc (document-level)\n","  \n","  Returns:\n","    silhouette score (float)\n","\n","  \"\"\"\n","  return metrics.silhouette_score(data, labels_pred)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2rbrotpZjO2i","colab_type":"code","colab":{}},"source":["def internalValidation(data, labels_pred, centers_ids):\n","  \"\"\"\n","  Returns the the internal validation coefficients.\n","\n","  Parameters:\n","    data(list): list of docs\n","    labels_pred: the predicted labels of each doc (document-level)\n","    center_ids(list): ids of the vectors of the cluster centers\n","\n","  Returns:\n","    (s_coefficient, s_Dbw)\n","  \"\"\"\n","  s_coefficient = silhoutteCoefficient(data, labels_pred)\n","  s_Dbw = s_Dbw_validity_index(data, labels_pred, centers_ids)\n","  return s_coefficient, s_Dbw\n","  \n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p2Agr_wKjeZI","colab_type":"code","colab":{}},"source":["def adjustedRandIndex(labels_true, labels_pred) -> float:\n","  \"\"\"\n","  Function calculates ARI, used to examine homogeinity and competeness.\n","  \n","  Parameters:\n","    labels_true(list): the true labels\n","    labels_pred(list): the predicted labels\n","\n","  Returns:\n","    ARI value\n","  \"\"\"\n","\n","  return metrics.adjusted_rand_score(labels_true, labels_pred)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sNCOzH50jlIT","colab_type":"code","colab":{}},"source":["def fowlkesMallowsIndex(labels_true, labels_pred) -> float:\n","  \"\"\"\n","  Function calculates FMI, used to examine recall and precision.\n","\n","  Parameters:\n","    labels_true(list): the true labels\n","    labels_pred(list): the predicted labels\n","\n","  Returns:\n","    Fowlkes-Mallows score\n","\n","  \"\"\"\n","\n","  return metrics.fowlkes_mallows_score(labels_true, labels_pred)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5oVgPRvGj7JC","colab_type":"code","colab":{}},"source":["def externalValidation(labels_true, labels_pred):\n","  \"\"\"\n","  Function calculates the results of all external evaluations.\n","\n","  Parameters:\n","    labels_true(list): the true labels\n","    labels_pred(list): the predicted labels\n","  \n","  Returns:\n","    (ari, fmi) \n","  \"\"\"\n","  fmi = fowlkesMallowsIndex(labels_true, labels_pred)\n","  ari = adjustedRandIndex(labels_true, labels_pred)\n","  return ari, fmi\n","  \n"],"execution_count":null,"outputs":[]}]}