{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"preprocessing.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNx9+R7u25QTdlv0aLpmj71"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"Q85giTrlFpcn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":108},"executionInfo":{"status":"ok","timestamp":1596022842019,"user_tz":-120,"elapsed":4927,"user":{"displayName":"Ja Trev","photoUrl":"","userId":"09437363370651811697"}},"outputId":"38ee952c-2dd0-467a-c117-189e864a26c0"},"source":["from nltk.stem.porter import PorterStemmer\n","from nltk.tokenize import word_tokenize\n","from nltk.stem.wordnet import WordNetLemmatizer\n","\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","\n","from nltk.corpus import stopwords \n","sw = stopwords.words(\"english\")\n","\n","from stop_words import get_stop_words\n","en_stop = get_stop_words('en')\n","\n","stop_words = sw + en_stop\n","stop_words.append('let')\n","stop_words.append('gon')\n","stop_words.append('dhe')\n","stop_words.extend(['car', 'like', 'got', \n","                   'get', 'one', 'well', \n","                   'back', 'bit', 'drive', \n","                   'look', 'see', 'good', \n","                   'quite', 'think', 'little', \n","                   'right', 'know', 'thing', 'want'])\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"z2hlwOPwM1P4","colab_type":"code","colab":{}},"source":["def preprocessing(docs:list, vocab=False) -> list:\n","  \"\"\"\n","  Document for processing a list of documents.\n","\n","  Returns:\n","  - if vocab == False:\n","    returns the preprocessed documents\n","    \n","  - if vocab == True:\n","    returns list of words (vocubalary)\n","\n","  \"\"\"\n","\n","  vocabulary = []\n","  new_docs = [None for _ in range(len(docs))]\n","  for i, doc in enumerate(docs):\n","    \n","    doc = doc.lower()\n","\n","    tkns = word_tokenize(doc)\n","\n","    # remove all tokens that are >= 3\n","    tkns = [w for w in tkns if len(w) > 2]\n","\n","    # remove stop words\n","    tkns = [w for w in tkns if w not in stop_words]\n","    tkns = [w for w in tkns if w not in [\"\\'re\", \"n\\'t\", \n","                                         \"n\\'t\", \"'ve\", \"really\"]]\n","\n","    # remove all tokens that are just digits\n","    tkns = [w for w in tkns if w.isalpha()]\n","\n","    # lemmatizing\n","    tkns = [WordNetLemmatizer().lemmatize(w) for w in tkns]\n","\n","    # stemming\n","    #tkns = [PorterStemmer().stem(w) for w in tkns]\n","\n","    # remove all words that are not nouns\n","    tkns = [w for (w, pos) in nltk.pos_tag(tkns) if pos in ['NN',\n","                                                            'NNP', \n","                                                            'NNS']]\n","    #print(tkns)\n","    new_docs[i] = list(tkns)\n","    vocabulary.extend(tkns)\n","\n","  if vocab:\n","    return list(set(vocabulary))\n","  else:\n","    return new_docs\n"],"execution_count":null,"outputs":[]}]}