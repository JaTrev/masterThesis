{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"vectorization.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOSy+2aMdZ7FohF5uz5ULHY"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"gP2f6yC6a1Cm","colab_type":"code","colab":{}},"source":["import math\n","\n","import numpy as np\n","from scipy.sparse import csr_matrix\n","import scipy.sparse\n","from sklearn.utils.extmath import safe_sparse_dot\n","from gensim.models import Word2Vec, KeyedVectors \n","from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","\n","from preprocessing import prepare_doc\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","from sklearn.feature_extraction.text import CountVectorizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kQYttmnfaqyM","colab_type":"code","colab":{}},"source":["def getW2V_model(data, embedding_dim=300, context_size=4, min_freq=4, \n","                 iterations=5, skip_gram=0):\n","  \"\"\"\n","  Function creates a word2vec model. \n","  \"\"\"\n","\n","  w2v_model = Word2Vec(size=embedding_dim, window=context_size, \n","                       min_count= min_freq, sg=skip_gram)\n","  w2v_model.build_vocab(data)\n","  w2v_model.train(data, total_examples=w2v_model.corpus_count, \n","                  epochs=iterations)\n","\n","  return w2v_model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sXeNRGMUZwC0","colab_type":"code","colab":{}},"source":["def getD2V_model (docs:list, vector_size=5, window=2, min_count=1, \n","                  dist_memory=1, seed=42):\n","  \"\"\"\n","  Function creates a doc2vec model.\n","  \"\"\"\n","  \n","  tagged_doc = [TaggedDocument(doc, [i]) for i, doc in enumerate(docs)]\n","  doc2vec_model = Doc2Vec(tagged_doc, vector_size=vector_size, \n","                          window=window, min_count=min_count, \n","                          workers=4, dm=dist_memory, seed=seed)\n","  return doc2vec_model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n2ByMXW2bd4J","colab_type":"code","colab":{}},"source":["def getTFIDF(data, max_df=0.95, min_df=2):\n","  \"\"\"\n","  Calculates the tf-idf for the diven data.\n","  \"\"\"\n","\n","  tfidf_vectorizer = TfidfVectorizer(analyzer=prepare_doc,\n","                                     max_df=max_df, min_df=min_df, \n","                                     stop_words='english')\n","  tfidf = tfidf_vectorizer.fit_transform(data)\n","\n","  return tfidf, tfidf_vectorizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G0kFlZqhuS2v","colab_type":"code","colab":{}},"source":["def getTF(data, max_df=0.95, min_df=2,):\n","  \"\"\"\n","  Calculates the term frequency for the given data.\n","  \"\"\"\n","  tf_vectorizer = CountVectorizer(max_df=max_df, \n","                                  min_df=min_df, stop_words='english')\n","  tf = tf_vectorizer.fit_transform(data)\n","\n","  return tf, tf_vectorizer \n"],"execution_count":null,"outputs":[]}]}